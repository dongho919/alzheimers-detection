{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f509f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "import pathlib\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e1dcc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['NonDemented', 'Demented']\n",
    "num_classes = len(class_names)\n",
    "batch_size = 32\n",
    "input_shape = (176, 208, 3)\n",
    "\n",
    "def process(image, label):\n",
    "    image /= 255.\n",
    "    label = tf.one_hot(label, num_classes)\n",
    "    return image, label\n",
    "\n",
    "def flip(image, label):\n",
    "    x = tf.image.random_flip_left_right(image)\n",
    "    images = [image, x]\n",
    "    labels = [label, label]\n",
    "    return images, labels\n",
    "\n",
    "def augment(image, label):\n",
    "    func = tf.numpy_function(flip, [image, label], [tf.float32, tf.float32])\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4942a0e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5121 files belonging to 2 classes.\n",
      "Found 1279 files belonging to 2 classes.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "1-th value returned by pyfunc_3 is int32, but expects float\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15280/1926508075.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_train_dataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_train_dataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15280/1926508075.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_train_dataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_train_dataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    759\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[1;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 744\u001b[1;33m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[0;32m    745\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2725\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2726\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2727\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2728\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2729\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6939\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6940\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6941\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6942\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: 1-th value returned by pyfunc_3 is int32, but expects float\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "train_dir = pathlib.Path('../Project/Alzheimer_s Dataset_binary/train')\n",
    "test_dir = pathlib.Path('../Project/Alzheimer_s Dataset_binary/test')\n",
    "\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        labels='inferred',\n",
    "        class_names=class_names,\n",
    "        batch_size=batch_size,\n",
    "        image_size=input_shape[0:2])\n",
    "\n",
    "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        test_dir,\n",
    "        labels='inferred',\n",
    "        class_names=class_names,\n",
    "        batch_size=batch_size,\n",
    "        image_size=input_shape[0:2])\n",
    "\n",
    "_train_dataset = train_dataset.map(process, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "_train_dataset = train_dataset.map(augment, num_parallel_calls=tf.data.experimental.AUTOTUNE).unbatch().unbatch()\n",
    "test_dataset = test_dataset.map(process, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_x = np.concatenate([x for x, y in _train_dataset])\n",
    "train_y = np.concatenate([y for x, y in _train_dataset])\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "batch_count = 0\n",
    "image_count = 0\n",
    "for images, labels in train_dataset:\n",
    "    if batch_count == 0:\n",
    "        print(np.shape(images))\n",
    "        print(np.shape(labels))\n",
    "        for i in range(np.shape(labels)[0]):\n",
    "            image = images[i]\n",
    "            fig.add_subplot(8,8,i+1)\n",
    "            plt.imshow(image)\n",
    "    batch_count += 1\n",
    "    image_count += np.shape(labels)[0]\n",
    "   \n",
    "plt.show()\n",
    "\n",
    "#image_count = 0\n",
    "#for image, label in train_dataset:\n",
    "#    if image_count < 32:\n",
    "#        fig.add_subplot(8,8,image_count+1)\n",
    "#        plt.imshow(image)\n",
    "#    image_count += 1\n",
    "   \n",
    "#plt.show()\n",
    "\n",
    "print(tf.data.experimental.cardinality(train_dataset))\n",
    "#print(f\"{image_count} images\")\n",
    "print(f\"{batch_count} batches, {image_count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "113667f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_15292/2426998501.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\dongh\\AppData\\Local\\Temp/ipykernel_15292/2426998501.py\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    zoom_range=0.2,\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "######################## deprecated ########################\n",
    "\n",
    "train_dir = pathlib.Path('../Project/Alzheimer_s Dataset_binary/train')\n",
    "test_dir = pathlib.Path('../Project/Alzheimer_s Dataset_binary/test')\n",
    "\n",
    "datagen = keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True,\n",
    "                                                       #rotation_range=10,\n",
    "                                                       rescale=1./255\n",
    "                                                       #brightness_range=(0.8,1.1),\n",
    "                                                       zoom_range=0.2,\n",
    "                                                       #width_shift_range=0.1,\n",
    "                                                       #height_shift_range=0.1\n",
    "                                                      )\n",
    "\n",
    "simple_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        classes=class_names,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        target_size=input_shape[0:2])\n",
    "\n",
    "test_generator = simple_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        classes=class_names,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        target_size=input_shape[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a372cbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 176, 208, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 176, 208, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 176, 208, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 88, 104, 64)       0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 88, 104, 128)      73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 88, 104, 128)      147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 44, 52, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 44, 52, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 44, 52, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 44, 52, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 44, 52, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 22, 26, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 22, 26, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 22, 26, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 22, 26, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 22, 26, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 11, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 11, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 11, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 11, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 11, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 5, 6, 512)         0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 0\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg = keras.applications.VGG19(\n",
    "        include_top=False,\n",
    "        input_shape=input_shape,\n",
    "        pooling=max)\n",
    "\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False\n",
    "#vgg.layers[17].trainable = True\n",
    "#vgg.layers[18].trainable = True\n",
    "#vgg.layers[19].trainable = True\n",
    "#vgg.layers[20].trainable = True\n",
    "\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9cee7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1 **(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(0.02, 10)\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"alzheimer_model.h5\",\n",
    "                                                    save_best_only=True)\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                     restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a8d8007",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gaussian_noise (GaussianNois (None, 176, 208, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg19 (Functional)           (None, 5, 6, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 15360)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               7864832   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 27,966,082\n",
      "Trainable params: 7,940,290\n",
      "Non-trainable params: 20,025,792\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "161/161 [==============================] - 30s 182ms/step - loss: 0.7082 - auc: 0.6411 - val_loss: 0.7384 - val_auc: 0.6193\n",
      "Epoch 2/30\n",
      "161/161 [==============================] - 29s 181ms/step - loss: 0.6158 - auc: 0.7264 - val_loss: 0.7724 - val_auc: 0.6405\n",
      "Epoch 3/30\n",
      "161/161 [==============================] - 29s 182ms/step - loss: 0.5631 - auc: 0.7826 - val_loss: 0.7781 - val_auc: 0.6501\n",
      "Epoch 4/30\n",
      "161/161 [==============================] - 29s 181ms/step - loss: 0.5291 - auc: 0.8123 - val_loss: 1.3929 - val_auc: 0.6396\n",
      "Epoch 5/30\n",
      "161/161 [==============================] - 29s 182ms/step - loss: 0.5081 - auc: 0.8292 - val_loss: 0.7820 - val_auc: 0.7028\n",
      "Epoch 6/30\n",
      "161/161 [==============================] - 29s 183ms/step - loss: 0.4821 - auc: 0.8499 - val_loss: 1.1501 - val_auc: 0.6350\n",
      "Epoch 7/30\n",
      "161/161 [==============================] - 31s 194ms/step - loss: 0.4675 - auc: 0.8592 - val_loss: 0.9350 - val_auc: 0.6471\n",
      "Epoch 8/30\n",
      "161/161 [==============================] - 30s 185ms/step - loss: 0.4406 - auc: 0.8764 - val_loss: 1.1407 - val_auc: 0.6362\n",
      "Epoch 9/30\n",
      "161/161 [==============================] - 30s 184ms/step - loss: 0.4332 - auc: 0.8815 - val_loss: 1.0925 - val_auc: 0.6349\n",
      "Epoch 10/30\n",
      "161/161 [==============================] - 29s 181ms/step - loss: 0.4205 - auc: 0.8888 - val_loss: 1.2479 - val_auc: 0.6370\n",
      "Epoch 11/30\n",
      "161/161 [==============================] - 29s 181ms/step - loss: 0.3989 - auc: 0.9009 - val_loss: 1.2657 - val_auc: 0.6391\n"
     ]
    }
   ],
   "source": [
    "#### Create a transfer learning model ####\n",
    "\n",
    "epochs = 30\n",
    "keras.backend.clear_session()\n",
    "gc.collect()\n",
    "model = keras.models.Sequential([\n",
    "        keras.Input(input_shape),\n",
    "        layers.GaussianNoise(0.08),\n",
    "        vgg,\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.7),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax'),\n",
    "    \n",
    "    ])\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[tf.keras.metrics.AUC(name='auc')])\n",
    "history = model.fit(train_generator,\n",
    "                    #steps_per_epoch=image_count // batch_size,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb, lr_scheduler],\n",
    "                    epochs=epochs,\n",
    "                    validation_data=test_generator\n",
    "                    #validation_steps=800 // batch_size\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205343a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create a CNN from scratch ####\n",
    "\n",
    "epochs = 30\n",
    "keras.backend.clear_session()\n",
    "gc.collect()\n",
    "model = keras.models.Sequential([\n",
    "        keras.Input(input_shape),\n",
    "        #layers.GaussianNoise(0.5),\n",
    "        #Creates the convolutional layer\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(16, kernel_size=(3,3), activation='relu'),\n",
    "        layers.Conv2D(16, kernel_size=(3,3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPool2D(),\n",
    "        \n",
    "        layers.SeparableConv2D(32, kernel_size=(3,3), activation='relu'),\n",
    "        layers.SeparableConv2D(32, kernel_size=(3,3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPool2D(),\n",
    "        \n",
    "        layers.SeparableConv2D(64, kernel_size=(3,3), activation='relu'),\n",
    "        layers.SeparableConv2D(64, kernel_size=(3,3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPool2D(),\n",
    "        \n",
    "        layers.SeparableConv2D(128, kernel_size=(3,3), activation='relu'),\n",
    "        layers.SeparableConv2D(128, kernel_size=(3,3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPool2D(),\n",
    "        \n",
    "        layers.SeparableConv2D(256, kernel_size=(3,3), activation='relu'),\n",
    "        layers.SeparableConv2D(256, kernel_size=(3,3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPool2D(),\n",
    "    \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(num_classes, activation='softmax'),\n",
    "    ])\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[tf.keras.metrics.AUC(name='auc'), 'accuracy'])\n",
    "history = model.fit(train_generator, batch_size=batch_size, epochs=epochs, validation_data=test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c455d000",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('VGG19 Transfer Learning w/ Gaussian(1) Accuracy with Dropout')\n",
    "\n",
    "plt.plot(history.history['accuracy'], label = \"training acc.\")\n",
    "plt.plot(history.history['val_accuracy'], label = \"validation acc.\")\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training acc.', 'validation acc.'], loc='upper left')\n",
    "\n",
    "plt.savefig(\"../Project/Result2/VGG19_MODEL_Pre_1_ACCURACY_adam_dropout.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4583d1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
